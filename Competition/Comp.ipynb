{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7daa3d2",
   "metadata": {
    "id": "e7daa3d2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "from nltk.tokenize import word_tokenize,TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Conv1D,Dropout,MaxPooling1D,GlobalMaxPool1D,Activation,Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cP-YG66LGCdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cP-YG66LGCdc",
    "outputId": "9fb17b69-781e-407c-eb04-dbb929a3db78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd31ce55",
   "metadata": {
    "id": "fd31ce55"
   },
   "outputs": [],
   "source": [
    "#%%  Used Function\n",
    "def Clean_sentences(sent):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    sent= re.sub('[^a-zA-Z]', ' ', sent)\n",
    "    tokens=word_tokenize(sent)\n",
    "    tokens=[lemmatizer.lemmatize(stemmer.stem(word.lower())) for word in tokens if word.lower() not in stopwords.words('english')and len(word)>1 and word.isalnum()]\n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e0d4c98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e0d4c98",
    "outputId": "aa44a78f-992a-4c6b-acc7-1da2c5a33a1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1998 entries, 0 to 1997\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Types      1998 non-null   object\n",
      " 1   Sentences  1998 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#%%  loading cleaning data\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "dataset=pd.read_csv('TrainData.csv',names=['Types','Sentences'])\n",
    "dataset.info()\n",
    "\n",
    "Types = dataset['Types'].values\n",
    "Sentences = dataset['Sentences'].values\n",
    "#worldlist=[]\n",
    "\n",
    "#for Sentence in Sentences :\n",
    "#        worldlist.append(Clean_sentences(Sentence))\n",
    "\n",
    "#print(np.max([len(ligne) for ligne in worldlist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d9583b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d9583b2",
    "outputId": "b8e48146-8321-47a7-eb4c-efae53477373"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 12:10:03,938 : INFO : loading Word2Vec object from word2vec.model\n",
      "2022-01-17 12:10:03,955 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2022-01-17 12:10:03,955 : INFO : setting ignored attribute cum_table to None\n",
      "2022-01-17 12:10:04,111 : INFO : Word2Vec lifecycle event {'fname': 'word2vec.model', 'datetime': '2022-01-17T12:10:04.111139', 'gensim': '4.1.2', 'python': '3.9.9 (main, Dec 16 2021, 23:13:29) \\n[GCC 11.2.0]', 'platform': 'Linux-5.15.0-kali2-amd64-x86_64-with-glibc2.33', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=18237, vector_size=300, alpha=0.025)\n",
      "18237\n"
     ]
    }
   ],
   "source": [
    "#%%    build word2vect model\n",
    "\n",
    "#w2v_model = gensim.models.Word2Vec(worldlist, vector_size=300, window=5, min_count=1, workers=10)\n",
    "#w2v_model.train(worldlist,total_examples=len(worldlist),epochs=10)\n",
    "# w2v_model.save(\"word2vec.model\")    \n",
    "\n",
    "w2v_model=gensim.models.Word2Vec.load('word2vec.model')\n",
    "print(w2v_model,len(w2v_model.wv.vectors),sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0132a3af",
   "metadata": {
    "id": "0132a3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=28581\n"
     ]
    }
   ],
   "source": [
    "#%% preprocessing data \n",
    "\n",
    "Keras_tk = Tokenizer(len(w2v_model.wv.vectors))\n",
    "Keras_tk.fit_on_texts(Sentences)\n",
    "\n",
    "X_train_sequence = Keras_tk.texts_to_sequences(Sentences)\n",
    "\n",
    "MAX_LENGTH = 2194\n",
    "\n",
    "X_train_sequence  = pad_sequences(X_train_sequence, MAX_LENGTH )\n",
    "\n",
    "print(\"Vocabulary size={}\".format(len(Keras_tk.word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0322e526",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0322e526",
    "outputId": "04abbb9f-7dcf-4da3-e88b-84524aa3c9b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1998, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Encoding Labels\n",
    "\n",
    "Encodelabels=LabelEncoder()\n",
    "\n",
    "Y= Encodelabels.fit_transform(Types)\n",
    "Y= to_categorical(Y)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "773e4406",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "773e4406",
    "outputId": "6bbb381e-e1d3-4ba9-e16b-df4388314df0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1698, 5), (300, 5))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%  Split train And Test data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(np.array(X_train_sequence), Y, train_size=0.85, stratify=Y)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "602f2726",
   "metadata": {
    "id": "602f2726"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 12:17:38.452039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-17 12:17:38.516034: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-01-17 12:17:38.516088: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-01-17 12:17:38.517293: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2194, 400)         11432400  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 2194, 64)          409664    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 548, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 548, 32)           32800     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 137, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4384)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1122560   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,998,709\n",
      "Trainable params: 12,998,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#%% Build CNN model using keras\n",
    "\n",
    "Keras_model = Sequential()\n",
    "Keras_model.add(Embedding(len(Keras_tk.word_index), 400, input_length=MAX_LENGTH))\n",
    "Keras_model.add(Conv1D(filters=64, kernel_size=16, padding='same', activation='relu'))\n",
    "Keras_model.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "Keras_model.add(Conv1D(filters=32, kernel_size=16, padding='same', activation='relu'))\n",
    "Keras_model.add(MaxPooling1D(pool_size=4))\n",
    "Keras_model.add(Flatten())\n",
    "Keras_model.add(Dense(256, activation='relu'))\n",
    "Keras_model.add(Dense(5, activation='softmax'))\n",
    "Keras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "Keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba6092b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "ba6092b0",
    "outputId": "da5f6c4a-f04e-4a77-98d6-8fea1d3e4397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "14/14 [==============================] - 86s 6s/step - loss: 0.0077 - accuracy: 0.9953 - val_loss: 0.2375 - val_accuracy: 0.8300\n",
      "Epoch 2/4\n",
      "14/14 [==============================] - 91s 7s/step - loss: 0.0026 - accuracy: 0.9982 - val_loss: 0.2569 - val_accuracy: 0.8333\n",
      "Epoch 3/4\n",
      "14/14 [==============================] - 84s 6s/step - loss: 6.7100e-04 - accuracy: 1.0000 - val_loss: 0.2362 - val_accuracy: 0.8400\n",
      "Epoch 4/4\n",
      "14/14 [==============================] - 82s 6s/step - loss: 5.3878e-04 - accuracy: 1.0000 - val_loss: 0.2637 - val_accuracy: 0.8333\n",
      "10/10 [==============================] - 3s 285ms/step - loss: 0.2637 - accuracy: 0.8333\n",
      "Accuracy: 83.33%\n"
     ]
    }
   ],
   "source": [
    "#%% training\n",
    "\n",
    "#Keras_model.fit(X_train, Y_train,validation_split=0.1,epochs=8,batch_size=100,verbose=1)\n",
    "Keras_model.fit(X_train, Y_train, batch_size=128, epochs=4, validation_data=(X_test, Y_test))\n",
    "Accuracy = Keras_model.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (Accuracy[1]*100))\n",
    "\n",
    "#Keras_model.save('TextClassification_95.33%.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d4e1186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.6672242e-10 1.9124096e-17 4.2319231e-04 ... 4.1487347e-06 7.6890962e-13\n",
      " 9.9999583e-01]\n"
     ]
    }
   ],
   "source": [
    "#%% Prediction\n",
    "predict = Keras_model.predict(X_test).ravel()\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a546b",
   "metadata": {},
   "source": [
    "###  Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f833474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909434e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_dataset=pd.read_csv('Testdata.csv',names=['ID','text'])\n",
    "Test_dataset.info()\n",
    "Test_dataset.head()\n",
    "\n",
    "text = Test_dataset['text'].values\n",
    "worldlist=[]\n",
    "\n",
    "for Sentence in text :\n",
    "        worldlist.append(Clean_sentences(Sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20c71d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 12:32:45,312 : INFO : loading Word2Vec object from word2vec.model\n",
      "2022-01-17 12:32:45,345 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2022-01-17 12:32:45,346 : INFO : setting ignored attribute cum_table to None\n",
      "2022-01-17 12:32:45,542 : INFO : Word2Vec lifecycle event {'fname': 'word2vec.model', 'datetime': '2022-01-17T12:32:45.542868', 'gensim': '4.1.2', 'python': '3.9.9 (main, Dec 16 2021, 23:13:29) \\n[GCC 11.2.0]', 'platform': 'Linux-5.15.0-kali2-amd64-x86_64-with-glibc2.33', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model=gensim.models.Word2Vec.load('word2vec.model')\n",
    "MAX_LENGTH = 2194\n",
    "\n",
    "Keras_tk = Tokenizer(len(w2v_model.wv.vectors))\n",
    "Keras_tk.fit_on_texts(text)\n",
    "\n",
    "X_test_sequence = Keras_tk.texts_to_sequences(text)\n",
    "X_test_sequence  = pad_sequences(X_test_sequence, MAX_LENGTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97d179ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228], 'Proba': ['94.57%', '70.23%', '61.85%', '6.78%', '18.47%', '2.92%', '31.58%', '59.01%', '96.53%', '79.91%', '89.65%', '27.61%', '21.51%', '61.83%', '70.78%', '13.78%', '62.54%', '7.38%', '40.73%', '2.08%', '67.76%', '99.03%', '91.99%', '2.01%', '90.90%', '50.80%', '3.82%', '98.45%', '9.90%', '87.45%', '85.38%', '17.10%', '89.92%', '33.24%', '5.29%', '75.76%', '62.05%', '1.11%', '82.22%', '21.39%', '76.19%', '11.91%', '88.39%', '50.84%', '87.67%', '22.97%', '21.44%', '99.62%', '7.40%', '75.29%', '9.43%', '13.01%', '88.45%', '61.38%', '12.21%', '3.70%', '89.10%', '11.74%', '6.02%', '26.64%', '3.74%', '10.81%', '96.93%', '7.68%', '24.93%', '24.30%', '0.38%', '13.76%', '5.13%', '0.39%', '12.66%', '3.98%', '31.21%', '53.23%', '4.02%', '8.61%', '58.43%', '21.19%', '5.24%', '74.44%', '17.04%', '27.73%', '17.04%', '7.35%', '43.94%', '74.96%', '58.91%', '44.99%', '0.04%', '41.23%', '3.85%', '16.80%', '36.59%', '55.41%', '99.41%', '68.07%', '19.37%', '49.98%', '95.10%', '58.18%', '9.22%', '6.62%', '17.98%', '67.88%', '3.24%', '29.09%', '18.92%', '53.61%', '74.90%', '7.69%', '22.98%', '19.29%', '8.67%', '56.56%', '99.92%', '81.98%', '58.25%', '41.32%', '15.30%', '0.26%', '1.11%', '43.75%', '65.11%', '81.08%', '70.21%', '6.29%', '70.11%', '18.79%', '19.47%', '34.72%', '2.98%', '71.34%', '3.12%', '48.61%', '1.59%', '0.18%', '48.61%', '4.47%', '92.49%', '1.23%', '4.52%', '1.85%', '29.56%', '45.98%', '99.99%', '63.54%', '70.22%', '44.77%', '5.61%', '16.79%', '31.28%', '74.32%', '80.83%', '53.04%', '97.42%', '16.04%', '0.45%', '3.97%', '18.93%', '45.56%', '20.75%', '93.34%', '15.95%', '0.23%', '1.96%', '2.36%', '77.52%', '5.59%', '12.00%', '5.91%', '8.81%', '6.62%', '99.27%', '61.03%', '5.52%', '28.82%', '49.47%', '79.77%', '41.38%', '8.42%', '1.46%', '5.15%', '20.52%', '3.62%', '5.72%', '2.24%', '3.17%', '43.57%', '1.06%', '72.59%', '5.68%', '49.70%', '7.51%', '46.72%', '27.71%', '9.06%', '1.17%', '2.86%', '4.96%', '28.14%', '91.29%', '3.97%', '30.49%', '40.81%', '4.77%', '72.78%', '51.52%', '29.35%', '20.14%', '11.91%', '3.57%', '47.16%', '5.55%', '73.32%', '50.49%', '3.36%', '2.52%', '33.01%', '11.16%', '70.54%', '3.58%', '17.38%', '32.38%', '54.61%', '8.39%', '55.14%', '0.25%', '70.50%'], 'Class': ['entertainment', 'sport', 'entertainment', 'politics', 'sport', 'sport', 'entertainment', 'sport', 'sport', 'politics', 'sport', 'politics', 'entertainment', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'entertainment', 'sport', 'tech', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'entertainment', 'entertainment', 'sport', 'entertainment', 'entertainment', 'sport', 'sport', 'business', 'politics', 'sport', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'entertainment', 'sport', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'sport', 'entertainment', 'entertainment', 'sport', 'sport', 'entertainment', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'entertainment', 'entertainment', 'sport', 'sport', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'entertainment', 'entertainment', 'business', 'sport', 'sport', 'sport', 'entertainment', 'entertainment', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'entertainment', 'sport', 'entertainment', 'sport', 'politics', 'sport', 'sport', 'sport', 'sport', 'entertainment', 'entertainment', 'sport', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'entertainment', 'entertainment', 'sport', 'sport', 'sport', 'sport', 'sport', 'business', 'sport', 'sport', 'entertainment', 'entertainment', 'sport', 'sport', 'politics', 'entertainment', 'sport', 'entertainment', 'entertainment', 'sport', 'politics', 'entertainment', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'entertainment', 'politics', 'sport', 'business', 'sport', 'politics', 'sport', 'entertainment', 'sport', 'entertainment', 'sport', 'sport', 'politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'politics', 'sport', 'entertainment', 'entertainment', 'business', 'sport', 'politics', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'politics', 'politics', 'politics', 'entertainment', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'entertainment', 'tech', 'sport', 'sport', 'entertainment']}\n"
     ]
    }
   ],
   "source": [
    "Keras_model = load_model('TextClassification_95.33%.h5')\n",
    "Probabilities=Keras_model.predict(X_test_sequence)\n",
    "\n",
    "Class_Dict={0:'business',1:'entertainment',2:'sport',3:'tech',4:'politics'}\n",
    "FinalResult={'ID':[],'Proba':[],'Class':[]}\n",
    "\n",
    "for i in range(len(Probabilities)):\n",
    "    maxProba=np.max(Probabilities[i])\n",
    "    Class=[j for j in range(len(Probabilities[i])) if Probabilities[i][j]==maxProba][0]\n",
    "    Class=Class_Dict[Class]\n",
    "    FinalResult['ID'].append(i+1)\n",
    "    FinalResult['Proba'].append(str(\"%.2f%%\" % (maxProba*100)))\n",
    "    FinalResult['Class'].append(Class)\n",
    "print(FinalResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({'ID': FinalResult['ID'],\n",
    "                   'Proba': FinalResult['Proba'],\n",
    "                   'Class':FinalResult['Class']\n",
    "                   })\n",
    "\n",
    "df.to_csv('YOUNES AMRI - Result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Comp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
